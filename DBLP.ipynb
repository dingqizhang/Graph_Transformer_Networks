{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b65a9d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e882afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d2cc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adaptive_lr='false', dataset='DBLP', epoch=40, lr=0.005, node_dim=64, norm='true', num_channels=2, num_layers=3, weight_decay=0.001)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str,\n",
    "                    help='Dataset')\n",
    "parser.add_argument('--epoch', type=int, default=40,\n",
    "                    help='Training Epochs')\n",
    "parser.add_argument('--node_dim', type=int, default=64,\n",
    "                    help='Node dimension')\n",
    "parser.add_argument('--num_channels', type=int, default=2,\n",
    "                    help='number of channels')\n",
    "parser.add_argument('--lr', type=float, default=0.005,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001,\n",
    "                    help='l2 reg')\n",
    "parser.add_argument('--num_layers', type=int, default=3,\n",
    "                    help='number of layer')\n",
    "parser.add_argument('--norm', type=str, default='true',\n",
    "                    help='normalization')\n",
    "parser.add_argument('--adaptive_lr', type=str, default='false',\n",
    "                    help='adaptive learning rate')\n",
    "\n",
    "args = parser.parse_args(['--dataset','DBLP','--num_layers','3'])\n",
    "print(args)\n",
    "epochs = args.epoch\n",
    "node_dim = args.node_dim\n",
    "num_channels = args.num_channels\n",
    "lr = args.lr\n",
    "weight_decay = args.weight_decay\n",
    "num_layers = args.num_layers\n",
    "norm = args.norm\n",
    "adaptive_lr = args.adaptive_lr\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248db8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d913bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768/374859228.py:5: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  edges = pickle.load(f)\n",
      "/tmp/ipykernel_768/374859228.py:5: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  edges = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "## open and load node features, egdes and labels\n",
    "with open('data/'+args.dataset+'/node_features.pkl','rb') as f:\n",
    "    node_features = pickle.load(f)\n",
    "with open('data/'+args.dataset+'/edges.pkl','rb') as f:\n",
    "    edges = pickle.load(f)\n",
    "with open('data/'+args.dataset+'/labels.pkl','rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c8a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the number of nodes\n",
    "num_nodes = edges[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82ea4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate each edge type's adjacency matrices, as well as the identity matrix\n",
    "for i,edge in enumerate(edges):\n",
    "    if i == 0:\n",
    "        A = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "    else:\n",
    "        A = torch.cat([A, torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)\n",
    "        \n",
    "A = torch.cat([A, torch.eye(num_nodes).type(torch.FloatTensor).unsqueeze(-1)], dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0af5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor).to(device)\n",
    "## creating trainig dataset\n",
    "train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.LongTensor).to(device)\n",
    "train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.LongTensor).to(device)\n",
    "\n",
    "## creating validation dataset\n",
    "valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.LongTensor).to(device)\n",
    "valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.LongTensor).to(device)\n",
    "\n",
    "## creating test dataset\n",
    "test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.LongTensor).to(device)\n",
    "test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.LongTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb95cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get number of classes\n",
    "num_classes = torch.max(train_target).item()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d894b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a83955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(GTConv, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(output_channels, input_channels, 1, 1))\n",
    "        self.bias = None\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = self.input_channels\n",
    "        nn.init.constant_(self.weight, 0.1)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "            \n",
    "    def forward(self, A):\n",
    "        attention_score = F.softmax(self.weight, dim=1)\n",
    "        A = torch.sum(A*attention_score, dim=1)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6bbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, first=True):\n",
    "        super(GTLayer, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.first = first\n",
    "        \n",
    "        if self.first == True:\n",
    "            self.conv1 = GTConv(input_channels, output_channels)\n",
    "            self.conv2 = GTConv(input_channels, output_channels)\n",
    "        else:\n",
    "            self.conv1 = GTConv(input_channels, output_channels)\n",
    "    \n",
    "    def forward(self, A, H_normalized=None):\n",
    "        if self.first == True:\n",
    "            Q1 = self.conv1(A)\n",
    "            Q2 = self.conv2(A)\n",
    "            H = torch.bmm(Q1,Q2)\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach(),(F.softmax(self.conv2.weight, dim=1)).detach()]\n",
    "        else:\n",
    "            Q1 = self.conv1(A)\n",
    "            H = torch.bmm(H_normalized,Q1)\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach()]\n",
    "        return H,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d53bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(nn.Module):\n",
    "    def __init__(self, num_edge, num_channels, w_in, w_out, num_class, num_layers, norm):\n",
    "        super(GTN, self).__init__()\n",
    "        self.num_edge = num_edge\n",
    "        self.num_channels = num_channels\n",
    "        self.w_in = w_in\n",
    "        self.w_out = w_out\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        self.is_norm = norm\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i==0:\n",
    "                layers.append(GTLayer(num_edge, num_channels, first=True))\n",
    "            else:\n",
    "                layers.append(GTLayer(num_edge, num_channels, first=False))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.weight = nn.Parameter(torch.Tensor(w_in, w_out))\n",
    "        self.bias = nn.Parameter(torch.Tensor(w_out))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.linear1 = nn.Linear(self.w_out*self.num_channels, self.w_out)\n",
    "        self.linear2 = nn.Linear(self.w_out, self.num_class)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def gcn_conv(self, H, X):\n",
    "        H_normalized = self.norm(H, add=True)\n",
    "        X = torch.mm(X, self.weight)\n",
    "        return torch.mm(H_normalized.transpose(0,1), X)\n",
    "\n",
    "    def normalization(self, H):\n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                H_normalized = self.norm(H[i,:,:]).unsqueeze(0)\n",
    "            else:\n",
    "                H_normalized = torch.cat((H_normalized, self.norm(H[i,:,:]).unsqueeze(0)), dim=0)\n",
    "        return H_normalized\n",
    "\n",
    "    def norm(self, H, add=False):\n",
    "        H = H.transpose(0,1)\n",
    "        if add == False:\n",
    "            H = H*((torch.eye(H.shape[0])==0).type(torch.FloatTensor)).to(device)\n",
    "        else:\n",
    "            H = H*((torch.eye(H.shape[0])==0).type(torch.FloatTensor)).to(device) + torch.eye(H.shape[0]).type(torch.FloatTensor).to(device)\n",
    "        deg = torch.sum(H, dim=1)\n",
    "        deg_inv = deg.pow(-1)\n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        deg_inv = deg_inv*torch.eye(H.shape[0]).type(torch.FloatTensor).to(device)\n",
    "        H = torch.mm(deg_inv,H)\n",
    "        H = H.t()\n",
    "        return H\n",
    "    \n",
    "    def forward(self, A, X, target_x, target):\n",
    "        A = A.unsqueeze(0).permute(0,3,1,2) \n",
    "        Ws = []\n",
    "        for i in range(self.num_layers):\n",
    "            if i==0:\n",
    "                H, W = self.layers[i](A)\n",
    "            else:\n",
    "                H = self.normalization(H)\n",
    "                H, W = self.layers[i](A, H)\n",
    "            Ws.append(W)\n",
    "            \n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                X_ = F.relu(self.gcn_conv(H[i], X))\n",
    "            else:\n",
    "                X_tmp = F.relu(self.gcn_conv(H[i], X))\n",
    "                X_ = torch.cat((X_, X_tmp), dim=1)\n",
    "                \n",
    "        X_ = F.relu(self.linear1(X_))\n",
    "        y = self.linear2(X_[target_x])\n",
    "        loss = self.loss(y, target)\n",
    "        return loss, y, Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509d7d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeeb2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    r\"\"\"Computes the accuracy of correct predictions.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true positive predictions.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true negative predictions.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false positive predictions.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false negative predictions.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    r\"\"\"Computes the precision:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}`.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    r\"\"\"Computes the recall:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}`.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    r\"\"\"Computes the :math:`F_1` score:\n",
    "    :math:`2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}\n",
    "    {\\mathrm{precision}+\\mathrm{recall}}`.\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c274b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e89204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train_Loss: 1.3837169408798218, Macro_F1: 0.1955912560224533\n",
      "Valid_Loss: 1.3601574897766113, Macro_F1: 0.4648205637931824\n",
      "Test_Loss: 1.3601841926574707, Macro_F1: 0.4343063533306122\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 1.3837169408798218, Macro_F1: 0.1955912560224533\n",
      "Valid_Loss: 1.3601574897766113, Macro_F1: 0.4648205637931824\n",
      "Test_Loss: 1.3601841926574707, Macro_F1: 0.4343063533306122\n",
      "\n",
      "Epoch: 2\n",
      "Train_Loss: 1.356652021408081, Macro_F1: 0.46221670508384705\n",
      "Valid_Loss: 1.326909065246582, Macro_F1: 0.5376998782157898\n",
      "Test_Loss: 1.3253332376480103, Macro_F1: 0.5233026742935181\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 1.356652021408081, Macro_F1: 0.46221670508384705\n",
      "Valid_Loss: 1.326909065246582, Macro_F1: 0.5376998782157898\n",
      "Test_Loss: 1.3253332376480103, Macro_F1: 0.5233026742935181\n",
      "\n",
      "Epoch: 3\n",
      "Train_Loss: 1.3200714588165283, Macro_F1: 0.5579690933227539\n",
      "Valid_Loss: 1.2767539024353027, Macro_F1: 0.6081326007843018\n",
      "Test_Loss: 1.2730919122695923, Macro_F1: 0.5868467688560486\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 1.3200714588165283, Macro_F1: 0.5579690933227539\n",
      "Valid_Loss: 1.2767539024353027, Macro_F1: 0.6081326007843018\n",
      "Test_Loss: 1.2730919122695923, Macro_F1: 0.5868467688560486\n",
      "\n",
      "Epoch: 4\n",
      "Train_Loss: 1.2663466930389404, Macro_F1: 0.6268446445465088\n",
      "Valid_Loss: 1.2061885595321655, Macro_F1: 0.7020045518875122\n",
      "Test_Loss: 1.202628493309021, Macro_F1: 0.6794014573097229\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 1.2663466930389404, Macro_F1: 0.6268446445465088\n",
      "Valid_Loss: 1.2061885595321655, Macro_F1: 0.7020045518875122\n",
      "Test_Loss: 1.202628493309021, Macro_F1: 0.6794014573097229\n",
      "\n",
      "Epoch: 5\n",
      "Train_Loss: 1.192763090133667, Macro_F1: 0.7285013794898987\n",
      "Valid_Loss: 1.1159636974334717, Macro_F1: 0.7822796106338501\n",
      "Test_Loss: 1.115262746810913, Macro_F1: 0.7659944891929626\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 1.192763090133667, Macro_F1: 0.7285013794898987\n",
      "Valid_Loss: 1.1159636974334717, Macro_F1: 0.7822796106338501\n",
      "Test_Loss: 1.115262746810913, Macro_F1: 0.7659944891929626\n",
      "\n",
      "Epoch: 6\n",
      "Train_Loss: 1.0990499258041382, Macro_F1: 0.8014441728591919\n",
      "Valid_Loss: 1.0072673559188843, Macro_F1: 0.8214226365089417\n",
      "Test_Loss: 1.0051313638687134, Macro_F1: 0.8324579000473022\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 1.0990499258041382, Macro_F1: 0.8014441728591919\n",
      "Valid_Loss: 1.0072673559188843, Macro_F1: 0.8214226365089417\n",
      "Test_Loss: 1.0051313638687134, Macro_F1: 0.8324579000473022\n",
      "\n",
      "Epoch: 7\n",
      "Train_Loss: 0.9868699908256531, Macro_F1: 0.8594939708709717\n",
      "Valid_Loss: 0.8842800855636597, Macro_F1: 0.8556625843048096\n",
      "Test_Loss: 0.8774334788322449, Macro_F1: 0.8664798736572266\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.9868699908256531, Macro_F1: 0.8594939708709717\n",
      "Valid_Loss: 0.8842800855636597, Macro_F1: 0.8556625843048096\n",
      "Test_Loss: 0.8774334788322449, Macro_F1: 0.8664798736572266\n",
      "\n",
      "Epoch: 8\n",
      "Train_Loss: 0.8613611459732056, Macro_F1: 0.874505877494812\n",
      "Valid_Loss: 0.7553816437721252, Macro_F1: 0.8683097958564758\n",
      "Test_Loss: 0.7521982789039612, Macro_F1: 0.8865856528282166\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.8613611459732056, Macro_F1: 0.874505877494812\n",
      "Valid_Loss: 0.7553816437721252, Macro_F1: 0.8683097958564758\n",
      "Test_Loss: 0.7521982789039612, Macro_F1: 0.8865856528282166\n",
      "\n",
      "Epoch: 9\n",
      "Train_Loss: 0.7306329607963562, Macro_F1: 0.9021008014678955\n",
      "Valid_Loss: 0.6296085715293884, Macro_F1: 0.8913813829421997\n",
      "Test_Loss: 0.633251965045929, Macro_F1: 0.8955656886100769\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.7306329607963562, Macro_F1: 0.9021008014678955\n",
      "Valid_Loss: 0.6296085715293884, Macro_F1: 0.8913813829421997\n",
      "Test_Loss: 0.633251965045929, Macro_F1: 0.8955656886100769\n",
      "\n",
      "Epoch: 10\n",
      "Train_Loss: 0.6041586995124817, Macro_F1: 0.9124352931976318\n",
      "Valid_Loss: 0.5121444463729858, Macro_F1: 0.9095598459243774\n",
      "Test_Loss: 0.5112919807434082, Macro_F1: 0.9158079624176025\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.6041586995124817, Macro_F1: 0.9124352931976318\n",
      "Valid_Loss: 0.5121444463729858, Macro_F1: 0.9095598459243774\n",
      "Test_Loss: 0.5112919807434082, Macro_F1: 0.9158079624176025\n",
      "\n",
      "Epoch: 11\n",
      "Train_Loss: 0.4889775812625885, Macro_F1: 0.9236735701560974\n",
      "Valid_Loss: 0.41334739327430725, Macro_F1: 0.9170295596122742\n",
      "Test_Loss: 0.4156914949417114, Macro_F1: 0.9203888177871704\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.4889775812625885, Macro_F1: 0.9236735701560974\n",
      "Valid_Loss: 0.41334739327430725, Macro_F1: 0.9170295596122742\n",
      "Test_Loss: 0.4156914949417114, Macro_F1: 0.9203888177871704\n",
      "\n",
      "Epoch: 12\n",
      "Train_Loss: 0.3904331922531128, Macro_F1: 0.9300497174263\n",
      "Valid_Loss: 0.33400020003318787, Macro_F1: 0.92982017993927\n",
      "Test_Loss: 0.3393160104751587, Macro_F1: 0.9229750633239746\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.3904331922531128, Macro_F1: 0.9300497174263\n",
      "Valid_Loss: 0.33400020003318787, Macro_F1: 0.92982017993927\n",
      "Test_Loss: 0.3393160104751587, Macro_F1: 0.9229750633239746\n",
      "\n",
      "Epoch: 13\n",
      "Train_Loss: 0.31133896112442017, Macro_F1: 0.9388749003410339\n",
      "Valid_Loss: 0.2733691930770874, Macro_F1: 0.9299067854881287\n",
      "Test_Loss: 0.27472779154777527, Macro_F1: 0.926414430141449\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.31133896112442017, Macro_F1: 0.9388749003410339\n",
      "Valid_Loss: 0.2733691930770874, Macro_F1: 0.9299067854881287\n",
      "Test_Loss: 0.27472779154777527, Macro_F1: 0.926414430141449\n",
      "\n",
      "Epoch: 14\n",
      "Train_Loss: 0.2515377104282379, Macro_F1: 0.93752521276474\n",
      "Valid_Loss: 0.23124785721302032, Macro_F1: 0.935056209564209\n",
      "Test_Loss: 0.23829515278339386, Macro_F1: 0.9288542866706848\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.2515377104282379, Macro_F1: 0.93752521276474\n",
      "Valid_Loss: 0.23124785721302032, Macro_F1: 0.935056209564209\n",
      "Test_Loss: 0.23829515278339386, Macro_F1: 0.9288542866706848\n",
      "\n",
      "Epoch: 15\n",
      "Train_Loss: 0.20777738094329834, Macro_F1: 0.9439513087272644\n",
      "Valid_Loss: 0.20102626085281372, Macro_F1: 0.9399474859237671\n",
      "Test_Loss: 0.2053966522216797, Macro_F1: 0.930353581905365\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.20777738094329834, Macro_F1: 0.9439513087272644\n",
      "Valid_Loss: 0.20102626085281372, Macro_F1: 0.9399474859237671\n",
      "Test_Loss: 0.2053966522216797, Macro_F1: 0.930353581905365\n",
      "\n",
      "Epoch: 16\n",
      "Train_Loss: 0.17564992606639862, Macro_F1: 0.9450172781944275\n",
      "Valid_Loss: 0.18071849644184113, Macro_F1: 0.9375101923942566\n",
      "Test_Loss: 0.19026102125644684, Macro_F1: 0.933515191078186\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.20777738094329834, Macro_F1: 0.9439513087272644\n",
      "Valid_Loss: 0.20102626085281372, Macro_F1: 0.9399474859237671\n",
      "Test_Loss: 0.2053966522216797, Macro_F1: 0.930353581905365\n",
      "\n",
      "Epoch: 17\n",
      "Train_Loss: 0.15106479823589325, Macro_F1: 0.9501105546951294\n",
      "Valid_Loss: 0.16817298531532288, Macro_F1: 0.9399474859237671\n",
      "Test_Loss: 0.1787286251783371, Macro_F1: 0.9338278770446777\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.20777738094329834, Macro_F1: 0.9439513087272644\n",
      "Valid_Loss: 0.20102626085281372, Macro_F1: 0.9399474859237671\n",
      "Test_Loss: 0.2053966522216797, Macro_F1: 0.930353581905365\n",
      "\n",
      "Epoch: 18\n",
      "Train_Loss: 0.13142743706703186, Macro_F1: 0.9513387680053711\n",
      "Valid_Loss: 0.160368874669075, Macro_F1: 0.9399619102478027\n",
      "Test_Loss: 0.17272797226905823, Macro_F1: 0.9334763288497925\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.13142743706703186, Macro_F1: 0.9513387680053711\n",
      "Valid_Loss: 0.160368874669075, Macro_F1: 0.9399619102478027\n",
      "Test_Loss: 0.17272797226905823, Macro_F1: 0.9334763288497925\n",
      "\n",
      "Epoch: 19\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 20\n",
      "Train_Loss: 0.10021047294139862, Macro_F1: 0.956418514251709\n",
      "Valid_Loss: 0.1550692766904831, Macro_F1: 0.9374359250068665\n",
      "Test_Loss: 0.17398707568645477, Macro_F1: 0.9351316690444946\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 21\n",
      "Train_Loss: 0.08648589253425598, Macro_F1: 0.9637824892997742\n",
      "Valid_Loss: 0.15639613568782806, Macro_F1: 0.9400240778923035\n",
      "Test_Loss: 0.1771932989358902, Macro_F1: 0.9350454211235046\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 22\n",
      "Train_Loss: 0.0735916718840599, Macro_F1: 0.9701130986213684\n",
      "Valid_Loss: 0.15738293528556824, Macro_F1: 0.9399616122245789\n",
      "Test_Loss: 0.18098056316375732, Macro_F1: 0.9357665777206421\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 23\n",
      "Train_Loss: 0.06169038265943527, Macro_F1: 0.9725362062454224\n",
      "Valid_Loss: 0.1616777777671814, Macro_F1: 0.9375000596046448\n",
      "Test_Loss: 0.1877913773059845, Macro_F1: 0.9329179525375366\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 24\n",
      "Train_Loss: 0.051190804690122604, Macro_F1: 0.975078821182251\n",
      "Valid_Loss: 0.16641376912593842, Macro_F1: 0.9350742101669312\n",
      "Test_Loss: 0.19532181322574615, Macro_F1: 0.9324541687965393\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 25\n",
      "Train_Loss: 0.04252377897500992, Macro_F1: 0.9850122928619385\n",
      "Valid_Loss: 0.1719047576189041, Macro_F1: 0.9350742101669312\n",
      "Test_Loss: 0.20244503021240234, Macro_F1: 0.9301705360412598\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 26\n",
      "Train_Loss: 0.035563115030527115, Macro_F1: 0.9887468218803406\n",
      "Valid_Loss: 0.1794399619102478, Macro_F1: 0.9325980544090271\n",
      "Test_Loss: 0.21011415123939514, Macro_F1: 0.9273314476013184\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 27\n",
      "Train_Loss: 0.030011320486664772, Macro_F1: 0.9912468791007996\n",
      "Valid_Loss: 0.18554408848285675, Macro_F1: 0.932632327079773\n",
      "Test_Loss: 0.21972832083702087, Macro_F1: 0.9266143441200256\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 28\n",
      "Train_Loss: 0.025466691702604294, Macro_F1: 0.9937468767166138\n",
      "Valid_Loss: 0.1927356719970703, Macro_F1: 0.932632327079773\n",
      "Test_Loss: 0.22873370349407196, Macro_F1: 0.9249281883239746\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 29\n",
      "Train_Loss: 0.021896708756685257, Macro_F1: 0.9962531328201294\n",
      "Valid_Loss: 0.20083005726337433, Macro_F1: 0.9326077103614807\n",
      "Test_Loss: 0.2369353175163269, Macro_F1: 0.9241054654121399\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 30\n",
      "Train_Loss: 0.019160829484462738, Macro_F1: 0.997499942779541\n",
      "Valid_Loss: 0.20743247866630554, Macro_F1: 0.9350828528404236\n",
      "Test_Loss: 0.24506597220897675, Macro_F1: 0.9233739972114563\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 31\n",
      "Train_Loss: 0.016878757625818253, Macro_F1: 0.997499942779541\n",
      "Valid_Loss: 0.21439871191978455, Macro_F1: 0.9350828528404236\n",
      "Test_Loss: 0.25307586789131165, Macro_F1: 0.9225108623504639\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 32\n",
      "Train_Loss: 0.015116634778678417, Macro_F1: 0.997499942779541\n",
      "Valid_Loss: 0.22191567718982697, Macro_F1: 0.9326085448265076\n",
      "Test_Loss: 0.2603371739387512, Macro_F1: 0.9215736389160156\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 33\n",
      "Train_Loss: 0.01367555744946003, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.22864417731761932, Macro_F1: 0.9276058077812195\n",
      "Test_Loss: 0.26663902401924133, Macro_F1: 0.9211751818656921\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 34\n",
      "Train_Loss: 0.01248523686081171, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.23445019125938416, Macro_F1: 0.922481894493103\n",
      "Test_Loss: 0.2720523774623871, Macro_F1: 0.9199119210243225\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 35\n",
      "Train_Loss: 0.01151087787002325, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.23992329835891724, Macro_F1: 0.917454719543457\n",
      "Test_Loss: 0.2771686315536499, Macro_F1: 0.9207201600074768\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 36\n",
      "Train_Loss: 0.010694991797208786, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.24497896432876587, Macro_F1: 0.917454719543457\n",
      "Test_Loss: 0.2816852033138275, Macro_F1: 0.9202297329902649\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 37\n",
      "Train_Loss: 0.010000965557992458, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.24935823678970337, Macro_F1: 0.917454719543457\n",
      "Test_Loss: 0.2854720950126648, Macro_F1: 0.9195597767829895\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 38\n",
      "Train_Loss: 0.009409034624695778, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.2530558109283447, Macro_F1: 0.9199637174606323\n",
      "Test_Loss: 0.28843432664871216, Macro_F1: 0.9185338616371155\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 39\n",
      "Train_Loss: 0.00888450164347887, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.2558933198451996, Macro_F1: 0.9199637174606323\n",
      "Test_Loss: 0.2910027503967285, Macro_F1: 0.9176114797592163\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n",
      "Epoch: 40\n",
      "Train_Loss: 0.008430873975157738, Macro_F1: 0.9987499713897705\n",
      "Valid_Loss: 0.25754988193511963, Macro_F1: 0.9199637174606323\n",
      "Test_Loss: 0.2929098606109619, Macro_F1: 0.9178862571716309\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train_Loss: 0.11495137214660645, Macro_F1: 0.9526035189628601\n",
      "Valid_Loss: 0.15641391277313232, Macro_F1: 0.9424853324890137\n",
      "Test_Loss: 0.1731013059616089, Macro_F1: 0.9369651675224304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GTN(num_edge=A.shape[-1],\n",
    "            num_channels=num_channels,\n",
    "            w_in=node_features.shape[1],\n",
    "            w_out=node_dim,\n",
    "            num_class=num_classes,\n",
    "            num_layers=num_layers,\n",
    "            norm=norm).to(device)\n",
    "\n",
    "if adaptive_lr == 'false':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam([{'params':model.weight},\n",
    "                                  {'params':model.linear1.parameters()},\n",
    "                                  {'params':model.linear2.parameters()},\n",
    "                                  {\"params\":model.layers.parameters(), \"lr\":0.5}\n",
    "                                  ], lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_train_loss = float(\"inf\")\n",
    "best_val_loss = float(\"inf\")\n",
    "best_test_loss = float(\"inf\")        \n",
    "best_train_f1 = 0\n",
    "best_val_f1 = 0\n",
    "best_test_f1 = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] > 0.005:\n",
    "            param_group['lr'] = param_group['lr'] * 0.9\n",
    "            \n",
    "    print('Epoch:',i+1)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    loss, y_train, Ws = model(A, node_features, train_node, train_target)\n",
    "    train_f1 = torch.mean(f1_score(torch.argmax(y_train.detach(),dim=1), train_target, num_classes=num_classes)).cpu().numpy()\n",
    "    print('Train_Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, y_valid,_ = model.forward(A, node_features, valid_node, valid_target)\n",
    "        val_f1 = torch.mean(f1_score(torch.argmax(y_valid.detach(),dim=1), valid_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Valid_Loss: {}, Macro_F1: {}'.format(val_loss.detach().cpu().numpy(), val_f1))\n",
    "        \n",
    "        test_loss, y_test, W = model.forward(A, node_features, test_node, test_target)\n",
    "        test_f1 = torch.mean(f1_score(torch.argmax(y_test.detach(),dim=1), test_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Test_Loss: {}, Macro_F1: {}\\n'.format(test_loss.detach().cpu().numpy(), test_f1))\n",
    "        \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_train_loss = loss.detach().cpu().numpy()\n",
    "        best_val_loss = val_loss.detach().cpu().numpy()\n",
    "        best_test_loss = test_loss.detach().cpu().numpy()\n",
    "\n",
    "        best_train_f1 = train_f1\n",
    "        best_val_f1 = val_f1\n",
    "        best_test_f1 = test_f1 \n",
    "    \n",
    "    print('---------------Best Results--------------------')\n",
    "    print('Train_Loss: {}, Macro_F1: {}'.format(best_train_loss, best_train_f1))\n",
    "    print('Valid_Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "    print('Test_Loss: {}, Macro_F1: {}\\n'.format(best_test_loss, best_test_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
